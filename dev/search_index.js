var documenterSearchIndex = {"docs":
[{"location":"api/#dtattreatments","page":"Api","title":"DataTreatments","text":"","category":"section"},{"location":"api/#windowing","page":"Api","title":"Windowing","text":"","category":"section"},{"location":"api/#DataTreatments.FeatureId","page":"Api","title":"DataTreatments.FeatureId","text":"FeatureId <: AbstractFeatureId\n\nA metadata container for individual features in a processed dataset.\n\nThis struct stores information about each feature column, including the source variable name, the transformation function applied, and the window number. It is designed for:\n\nExperiment documentation: All feature extraction parameters are preserved for reproducibility\nFeature selection: Metadata enables intelligent feature filtering and selection strategies\nTraceability: Each feature can be traced back to its source variable and transformation\n\nFields\n\nvname::Symbol: Source variable name from the original dataset\nfeat::Base.Callable: Feature extraction function (e.g., mean, std, maximum)\nnwin::Int64: Window number (1 for single window, >1 for multiple windows)\n\nExamples\n\n# Single window feature\nfid = FeatureId(:temperature, mean, 1)\n# Displays as: mean(temperature)\n\n# Multi-window feature\nfid = FeatureId(:pressure, maximum, 3)\n# Displays as: maximum(pressure)_w3\n\n# Access metadata\nget_vname(fid)    # :pressure\nget_feat(fid)  # maximum\nget_nwin(fid)     # 3\n\nSee Also\n\nDataTreatment: Main container using FeatureId for metadata\n\n\n\n\n\n","category":"type"},{"location":"api/#DataTreatments.DataTreatment","page":"Api","title":"DataTreatments.DataTreatment","text":"DataTreatment{T, S} <: AbstractDataTreatment\n\nA container for processed multidimensional data with complete metadata for reproducibility.\n\nThis struct stores the transformed dataset along with all processing parameters, ensuring full experiment documentation and reproducibility.\n\nFields\n\ndataset::AbstractMatrix{T}: Processed flat feature matrix (samples Ã— features)\nfeatureid::Vector{FeatureId}: Metadata for each feature column (enables feature selection)\nreducefunc::Base.Callable: Reduction function used (for :reducesize mode)\naggrtype::Symbol: Processing type (:aggregate or :reducesize)\n\nType Parameters\n\nT: Element type of the output dataset\nS: Core element type for nested structures\n\nConstructor\n\nDataTreatment(\n    X::Union{AbstractMatrix, AbstractDataFrame},\n    aggrtype::Symbol;\n    vnames::Vector{<:ValidVnames},\n    win::Union{Base.Callable, Tuple{Vararg{Base.Callable}}},\n    features::Tuple{Vararg{Base.Callable}}=(maximum, minimum, mean),\n    reducefunc::Base.Callable=mean,\n)\n\nArguments\n\nX: Input data (Matrix or DataFrame with multidimensional elements)\naggrtype: Processing mode (:aggregate or :reducesize)\nvnames: Variable names for feature identification\nwin: Window function(s) for data partitioning\nfeatures: Tuple of statistical functions to apply (default: (maximum, minimum, mean))\nreducefunc: Reduction function for :reducesize mode (default: mean)\n\nProcessing Modes\n\n:aggregate Mode\n\nTransforms the dataset from multi-dimensional to tabular format.\n\nThe dataset is windowed to reduce its dimensionality\nReduction functions from the features parameter are applied to each window (default: mean, maximum)\n\nXmatrix = fill(rand(200, 120), 100, 10)  # 100 samples, 10 variables\nwin = splitwindow(nwindows=4)\nfeatures = (mean, std, maximum)\n\ndt = DataTreatment(Xmatrix, :aggregate; \n                   vnames=Symbol.(\"var\", 1:10);\n                   win, \n                   features)\n# Returns 100Ã—(10Ã—3Ã—16) = 100Ã—480 flat matrix\n# 10 vars Ã— 3 features Ã— 16 windows (4Ã—4 grid)\n\n:reducesize Mode\n\nReduces the dataset dimensionality by windowing.\n\nOnce windowed, a reduction function called reducefunc (default: mean) is applied to each window\nNote: it is still possible to specify features as in :aggregate, but these will simply be saved for future use (as in modal algorithms like ModalDecisionTrees)\n\nXmatrix = fill(rand(200, 120), 100, 10)  # 100 samples, 10 variables\nwin = splitwindow(nwindows=4)\nfeatures = (mean, std, maximum)\n\ndt = DataTreatment(Xmatrix, :reducesize; \n                   vnames=Symbol.(\"var\", 1:10);\n                   win, \n                   features)\n# Each 200Ã—120 element becomes 4Ã—4, resulting in 100Ã—10 output\n\nGrouping\n\ngroups Parameter\n\nOptional parameter to group dataset elements before processing.\n\nAccepts a tuple of symbols specifying grouping columns\nCreates logical groups within the dataset for separate processing\nCommon grouping strategies: (:vname,), (:vname, :feat), (:vname, :timestamp)\n\ndt = DataTreatment(Xts, :aggregate;\n                   win=splitwindow(nwindows=2),\n                   features=(mean, maximum),\n                   groups=(:vname, :feat))\n# Processes each (vname, feat) group independently\n\nNormalization\n\nnorm Parameter\n\nOptional normalization function to apply during processing.\n\nAccepts a normalization function (e.g., zscore, minmax, center)\nApplied after windowing and feature reduction\nCan be created with keyword arguments for customization\n\n# Min-max normalization with custom range\ndt = DataTreatment(Xts, :aggregate;\n                   win=splitwindow(nwindows=2),\n                   features=(mean, maximum),\n                   norm=DT.minmax(lower=0.0, upper=1.0))\n\n# Z-score normalization\ndt = DataTreatment(Xts, :aggregate;\n                   win=splitwindow(nwindows=2),\n                   features=(mean, maximum),\n                   norm=DT.zscore())\n\nGrouped Normalization\n\nWhen using groups with norm, never specify dims parameter.\n\nGrouped normalization works on all elements of each group as a whole\nThe dims parameter should NOT be used: it operates column-wise or row-wise, which breaks group semantics\nEach group is normalized independently using all its data\n\n# CORRECT: Grouped normalization without dims\ngroups = DT.groupby(X, [[:var1, :var2]])\nnormalized = DT.normalize(groups, DT.zscore())\n# Each group normalized using all its elements\n\n# INCORRECT: Do not use dims with grouped normalization\n# normalized = DT.normalize(groups, DT.zscore(dims=2))\n# This breaks the group semantics!\n\nExamples\n\nBasic Usage with DataFrame\n\nusing DataFrames\n\n# Create dataset with multidimensional elements\ndf = DataFrame(\n    channel1 = [rand(200, 120) for _ in 1:1000],\n    channel2 = [rand(200, 120) for _ in 1:1000],\n    channel3 = [rand(200, 120) for _ in 1:1000]\n)\n\n# Define processing parameters\nwin = adaptivewindow(nwindows=6, overlap=0.15)\nfeatures = (mean, std, maximum, minimum, median)\n\n# Process to tabular format\ndt = DataTreatment(df, :reducesize; win, features)\n\n# Access processed data\nX_flat = get_dataset(dt)        # Flat feature matrix\nfeature_ids = get_featureid(dt) # Feature metadata\n\nFeature Selection Using Metadata\n\n# Get all feature metadata\nfeature_ids = get_featureid(dt)\n\n# Select specific features\nmean_features = findall(fid -> get_feature(fid) == mean, feature_ids)\nX_means = dt.dataset[:, mean_features]\n\n# Select features from specific variable\nch1_features = findall(fid -> get_vname(fid) == :channel1, feature_ids)\nX_ch1 = dt.dataset[:, ch1_features]\n\n# Select features from specific windows\nearly_windows = findall(fid -> get_nwin(fid) <= 3, feature_ids)\nX_early = dt.dataset[:, early_windows]\n\nReproducibility and Documentation\n\n# All parameters are stored for experiment reproduction\ndt = DataTreatment(df, :reducesize; win=(win,), features=features)\n\n# Extract processing metadata\naggrtype = get_aggrtype(dt)       # :reducesize\nreduction = get_reducefunc(dt)    # mean\nvar_names = get_vnames(dt)        # [:channel1, :channel2, :channel3]\nfeat_funcs = get_features(dt)     # (mean, std, maximum, minimum, median)\nn_windows = get_nwindows(dt)      # 6\n\n# Document experiment\nprintln(\"Processing: $aggrtype mode\")\nprintln(\"Variables: $(join(var_names, \", \"))\")\nprintln(\"Features: $(join(nameof.(feat_funcs), \", \"))\")\nprintln(\"Windows: $n_windows per dimension\")\n\nAccessor Functions\n\nget_dataset(dt): Extract the processed feature matrix\nget_featureid(dt): Get feature metadata vector\nget_reducefunc(dt): Get the reduction function used\nget_aggrtype(dt): Get the processing mode\nget_vnames(dt): Get unique variable names\nget_features(dt): Get unique feature functions\nget_nwindows(dt): Get maximum window number\n\nIndexing\n\nDataTreatment supports array-like indexing:\n\ndt[1, :]      # First sample (row)\ndt[:, 1]      # First feature (column)\ndt[1:10, :]   # First 10 samples\nsize(dt)      # Dataset dimensions\nlength(dt)    # Number of features\n\nSee Also\n\nFeatureId: Individual feature metadata\n@evalwindow: Window evaluation macro\n\n\n\n\n\n","category":"type"},{"location":"api/#DataTreatments.movingwindow","page":"Api","title":"DataTreatments.movingwindow","text":"movingwindow(; winsize::Int64[, winstep::Int64]) -> Function\n\nCreates a moving (sliding) window function with fixed window size and step.\n\nKeyword Arguments\n\nwinsize::Int64: Size of each window\nwinstep::Int64: Step between consecutive windows (defaults to winsize if 0)\n\nReturns\n\nFunction: A function that takes npoints::Int64 and returns Vector{UnitRange{Int64}}\n\nExample\n\nwfunc = movingwindow(winsize=10, winstep=5)\nwindows = wfunc(100)  # apply to sequence of length 100\n\nUse with macro\n\nA = rand(200)\nwindows = @evalwindow A movingwindow(winsize=10, winstep=5)\n\n\n\n\n\n","category":"function"},{"location":"api/#DataTreatments.wholewindow","page":"Api","title":"DataTreatments.wholewindow","text":"wholewindow() -> Function\n\nCreates a window function that returns a single window covering the entire sequence.\n\nReturns\n\nFunction: A function that takes npoints::Int64 and returns a single window [1:npoints]\n\nExample\n\nwfunc = wholewindow()\nwindows = wfunc(100)  # returns [1:100]\n\nUse with macro\n\nA = rand(200)\nwindows = @evalwindow A wholewindow()\n\n\n\n\n\n","category":"function"},{"location":"api/#DataTreatments.splitwindow","page":"Api","title":"DataTreatments.splitwindow","text":"splitwindow(; nwindows::Int64) -> Function\n\nCreates a window function that splits the sequence into a fixed number of non-overlapping windows.\n\nKeyword Arguments\n\nnwindows::Int64: Number of windows to create\n\nReturns\n\nFunction: A function that takes npoints::Int64 and returns Vector{UnitRange{Int64}}\n\nExample\n\nwfunc = splitwindow(nwindows=5)\nwindows = wfunc(100)  # splits into 5 equal windows\n\nUse with macro\n\nA = rand(200)\nwindows = @evalwindow A splitwindow(nwindows=5)\n\n\n\n\n\n","category":"function"},{"location":"api/#DataTreatments.adaptivewindow","page":"Api","title":"DataTreatments.adaptivewindow","text":"adaptivewindow(; nwindows::Int64[, overlap::Float64=0.0]) -> Function\n\nCreates a window function that adaptively divides the sequence into windows with optional overlap.\n\nKeyword Arguments\n\nnwindows::Int64: Number of windows to create\noverlap::Float64: Relative overlap between windows (0.0 to 1.0), defaults 0.0.\n\nReturns\n\nFunction: A function that takes npoints::Int64 and returns Vector{UnitRange{Int64}}\n\nExample\n\nwfunc = adaptivewindow(nwindows=5, overlap=0.2)\nwindows = wfunc(100)  # 5 windows with 20% overlap\n\nUse with macro\n\nA = rand(200)\nwindows = @evalwindow A adaptivewindow(nwindows=5, overlap=0.2)\n\n\n\n\n\n","category":"function"},{"location":"api/#DataTreatments.@evalwindow","page":"Api","title":"DataTreatments.@evalwindow","text":"@evalwindow(X, winfuncs...) -> Tuple{Vararg{Vector{UnitRange{Int64}}}}\n\nApply window functions to each dimension of an array.\n\nThis macro evaluates window functions for each dimension of an array, automatically  determining the appropriate number of points from the array's size. If fewer window  functions are provided than dimensions, the last function is reused for remaining dimensions.\n\nArguments\n\nX: Input array whose dimensions determine window parameters\nwinfuncs...: One or more window functions (e.g., movingwindow(), splitwindow(), adaptivewindow())\n\nReturns\n\nTuple{Vararg{Vector{UnitRange{Int64}}}}: Tuple of vectors containing window ranges for each dimension\n\nBehavior\n\nEach window function is applied to the corresponding dimension size\nIf length(winfuncs) < ndims(X), the last function is repeated for remaining dimensions\nWindow functions must be created using movingwindow, splitwindow, adaptivewindow, or wholewindow\n\nExamples\n\nSingle dimension\n\nX = rand(200)\nwindows = @evalwindow X movingwindow(winsize=50, winstep=25)\n\nTwo dimensions with different windows\n\nX = rand(200, 120)\nwindows = @evalwindow X splitwindow(nwindows=4) adaptivewindow(nwindows=3, overlap=0.1)\n# First dimension: 4 non-overlapping windows\n\nReusing window function for multiple dimensions\n\nX = rand(100, 120, 80)\nwindows = @evalwindow X splitwindow(nwindows=5)\n\nSee Also\n\nmovingwindow: Fixed-size sliding windows\nsplitwindow: Equal non-overlapping windows\nadaptivewindow: Windows with overlap\nwholewindow: Single window covering entire dimension\n\n\n\n\n\n","category":"macro"},{"location":"featureset/#featuresets","page":"FeatureSet","title":"Featuresets","text":"","category":"section"},{"location":"featureset/#Basic-Statistics","page":"FeatureSet","title":"Basic Statistics","text":"Standard statistical measures: maximum, minimum, mean, median, std, cov","category":"section"},{"location":"featureset/#Catch22-Features","page":"FeatureSet","title":"Catch22 Features","text":"Canonical time-series characteristics covering:\n\nDistribution properties and extreme events\nLinear and nonlinear autocorrelation structures  \nForecasting performance and scaling properties\nSymbolic dynamics and transition patterns","category":"section"},{"location":"featureset/#Predefined-Feature-Sets","page":"FeatureSet","title":"Predefined Feature Sets","text":"base_set: Minimal statistical features (4 features)\ncatch9: Curated subset combining statistics + key Catch22 (9 features)  \ncatch22_set: Complete Catch22 suite (22 features)\ncomplete_set: All features combined (28 features)","category":"section"},{"location":"featureset/#References","page":"FeatureSet","title":"References","text":"The Catch22 features are based on the Canonical Time-series Characteristics:\n\nRepository: https://github.com/DynamicsAndNeuralSystems/catch22\nPaper: Lubba, C.H., Sethi, S.S., Knaute, P. et al. \"catch22: CAnonical Time-series CHaracteristics.\" Data Min Knowl Disc 33, 1821â€“1852 (2019). https://doi.org/10.1007/s10618-019-00647-x","category":"section"},{"location":"featureset/#All-Catch22-Features","page":"FeatureSet","title":"All Catch22 Features","text":"","category":"section"},{"location":"featureset/#DataTreatments.base_set","page":"FeatureSet","title":"DataTreatments.base_set","text":"base_set\n\nA minimal feature set containing only basic statistical measures for time series analysis.\n\nFeatures\n\nmaximum: Maximum value in the time series\nminimum: Minimum value in the time series  \nmean   : Arithmetic mean of the time series\nstd    : Standard deviation of the time series\n\n\n\n\n\n","category":"constant"},{"location":"featureset/#DataTreatments.catch9","page":"FeatureSet","title":"DataTreatments.catch9","text":"catch9\n\nA curated subset of 9 features combining basic statistics with Symbolic Catch22 measures,\n\nFeatures\n\nBasic statistics: maximum, minimum, mean, median, std\nSymbolic Catch22 features:\nstretch_high       : Measures persistence of high values\nstretch_decreasing : Captures decreasing trend patterns\nentropy_pairs      : Quantifies local pattern complexity\ntransition_variance: Measures state transition variability\n\nReferences\n\nThe Catch22 features are based on the CAnonical Time-series CHaracteristics from:\n\nRepository: https://github.com/DynamicsAndNeuralSystems/catch22\nArticle:    https://doi.org/10.1007/s10618-019-00647-x\nAuthor: Carl H. Lubba et al\n\n\n\n\n\n","category":"constant"},{"location":"featureset/#DataTreatments.catch22_set","page":"FeatureSet","title":"DataTreatments.catch22_set","text":"catch22_set\n\nThe complete Catch22 feature set. Each feature captures different aspects of time series dynamics including correlation structure, distribution properties and temporal patterns.\n\nFeature Categories\n\nDistribution shape:  mode_5, mode_10\nExtreme event timing: outlier_timing_pos, outlier_timing_neg\nLinear autocorrelation: acf_timescale, acf_first_min, low_freq_power, centroid_freq\nSimple forecasting: forecast_error\nIncremental differences: whiten_timescale, high_fluctuation\nSymbolic stretch_high, stretch_decreasing, entropy_pairs, transition_variance\nNonlinear autocorrelation: ami2, trev\nLinear autocorrelation structure: ami_timescale, periodicity\nSelf-affine scaling: rs_range, dfa\nOther: embedding_dist\n\nReferences\n\nThe Catch22 features are based on the CAnonical Time-series CHaracteristics from:\n\nRepository: https://github.com/DynamicsAndNeuralSystems/catch22\nArticle:    https://doi.org/10.1007/s10618-019-00647-x\nAuthor: Carl H. Lubba et al\n\n\n\n\n\n","category":"constant"},{"location":"featureset/#DataTreatments.complete_set","page":"FeatureSet","title":"DataTreatments.complete_set","text":"complete_set\n\nThe most comprehensive feature set combining basic statistical measures, covariance analysis, and the full Catch22 suite.\n\nFeatures\n\nBasic statistics: maximum, minimum, mean, median, std, cov\nDistribution shape:  mode_5, mode_10\nExtreme event timing: outlier_timing_pos, outlier_timing_neg\nLinear autocorrelation: acf_timescale, acf_first_min, low_freq_power, centroid_freq\nSimple forecasting: forecast_error\nIncremental differences: whiten_timescale, high_fluctuation\nSymbolic stretch_high, stretch_decreasing, entropy_pairs, transition_variance\nNonlinear autocorrelation: ami2, trev\nLinear autocorrelation structure: ami_timescale, periodicity\nSelf-affine scaling: rs_range, dfa\nOther: embedding_dist\n\n\n\n\n\n","category":"constant"},{"location":"featureset/#DataTreatments.mode_5","page":"FeatureSet","title":"DataTreatments.mode_5","text":"DN_HistogramMode_5(x::AbstractVector{Union{Float64, Int}}) # For example\n\nAn alternative to catch22(:DN_HistogramMode_5](x). All features, such as DN_HistogramMode_5, are exported as Features and can be evaluated by calling their names.\n\nExamples\n\nð± = Catch22.testdata[:test]\nf = DN_HistogramMode_5(ð±)\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.mode_10","page":"FeatureSet","title":"DataTreatments.mode_10","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.embedding_dist","page":"FeatureSet","title":"DataTreatments.embedding_dist","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.acf_timescale","page":"FeatureSet","title":"DataTreatments.acf_timescale","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.acf_first_min","page":"FeatureSet","title":"DataTreatments.acf_first_min","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.ami2","page":"FeatureSet","title":"DataTreatments.ami2","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.trev","page":"FeatureSet","title":"DataTreatments.trev","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.outlier_timing_pos","page":"FeatureSet","title":"DataTreatments.outlier_timing_pos","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.outlier_timing_neg","page":"FeatureSet","title":"DataTreatments.outlier_timing_neg","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.whiten_timescale","page":"FeatureSet","title":"DataTreatments.whiten_timescale","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.forecast_error","page":"FeatureSet","title":"DataTreatments.forecast_error","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.ami_timescale","page":"FeatureSet","title":"DataTreatments.ami_timescale","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.high_fluctuation","page":"FeatureSet","title":"DataTreatments.high_fluctuation","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.stretch_decreasing","page":"FeatureSet","title":"DataTreatments.stretch_decreasing","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.stretch_high","page":"FeatureSet","title":"DataTreatments.stretch_high","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.entropy_pairs","page":"FeatureSet","title":"DataTreatments.entropy_pairs","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.rs_range","page":"FeatureSet","title":"DataTreatments.rs_range","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.dfa","page":"FeatureSet","title":"DataTreatments.dfa","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.low_freq_power","page":"FeatureSet","title":"DataTreatments.low_freq_power","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.centroid_freq","page":"FeatureSet","title":"DataTreatments.centroid_freq","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.transition_variance","page":"FeatureSet","title":"DataTreatments.transition_variance","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"featureset/#DataTreatments.periodicity","page":"FeatureSet","title":"DataTreatments.periodicity","text":"No documentation found for public binding TimeseriesFeatures.SuperFeatures.SuperFeature.\n\nSummary\n\nstruct TimeseriesFeatures.SuperFeatures.SuperFeature{F, G}\n\nFields\n\nfeature     :: F\nsuper       :: G\nname        :: Symbol\ndescription :: String\nkeywords    :: Vector{String}\n\nSupertype Hierarchy\n\nTimeseriesFeatures.SuperFeatures.SuperFeature{F, G} <: TimeseriesFeatures.SuperFeatures.AbstractSuperFeature <: TimeseriesFeatures.Features.AbstractFeature <: Function <: Any\n\n\n\n\n\n","category":"function"},{"location":"normalization/#Normalization","page":"Normalization","title":"Normalization","text":"","category":"section"},{"location":"normalization/#dtattreatments","page":"Normalization","title":"Methods Api","text":"","category":"section"},{"location":"normalization/#DataTreatments.normalize","page":"Normalization","title":"DataTreatments.normalize","text":"normalize(X, nfunc; tabular=false, dims=0)\nnormalize(df::DataFrame, nfunc; tabular=false, dims=0)\nnormalize(df::Vector{DataFrame}, nfunc; tabular=false, dims=0)\nnormalize!(X, nfunc; tabular=false, dims=0)\n\nApply a normalization function to data.\n\nSupported inputs\n\nAbstractArray{<:Real}: any numeric array (vector, matrix, or Nâ€‘D array)\nAbstractArray{<:AbstractArray{<:Real}}: nested arrays (e.g., dataset of vectors or matrices)\nDataFrame: tabular data with numeric columns (normalized columnâ€‘wise by default, dims=2)\nVector{DataFrame}: multiple dataframes to normalize together\n\nThe normalize variants allocate a new array; normalize! mutates in place.\n\nAvailable normalization methods\n\nzscore: Standardize by centering and scaling (mean=0, std=1)\nsigmoid: Map data to (0, 1) using logistic function\npnorm: Scale by p-norm (L1, L2, Lâˆž, etc.)\nscale: Scale by characteristic measure (std, MAD, IQR, first element)\nminmax: Rescale to specified range [lower, upper]\ncenter: Shift to zero mean or median\nunitpower: Scale to unit RMS power\noutliersuppress: Cap outliers beyond threshold\n\nWholeâ€‘dataset normalization (default)\n\nBy default, dims=0, the normalization function is computed on the entire dataset (flattened) and then applied elementâ€‘wise. This works for:\n\nNâ€‘D arrays (e.g., tensors)\narrays of vectors (audio segments)\narrays of matrices (images)\n\nTabular normalization (row/columnâ€‘wise: dims=1 or dims=2)\n\nFor tabular data (rows/columns as samples/features), set dims to the dimension:\n\ndims=1: normalize each row independently\ndims=2: normalize each column independently (typical: each column is a feature)\n\nExamples\n\nX = rand(100, 50)\n\n# Wholeâ€‘dataset zscore\nX_all = DataTreatments.normalize(X, zscore())\n\n# Columnâ€‘wise zscore (each feature independently)\nX_col = DataTreatments.normalize(X, zscore(); dims=2)\n\n# Rowâ€‘wise minmax\nX_row = DataTreatments.normalize(X, minmax(); dims=1)\n\n# Nested dataset (e.g., images)\nimgs = [rand(28,28) for _ in 1:100]\nimgs_norm = DataTreatments.normalize(imgs, unitpower())\n\nusing DataFrames\n\ndf = DataFrame(\n    age = [25, 30, 35, 40, 45],\n    salary = [50000, 60000, 75000, 80000, 90000],\n    score = [85, 90, 88, 92, 87]\n)\n\n# Normalize all numeric columns independently (column-wise)\ndf_norm = DataTreatments.normalize(df, zscore())\n\n# Aggregate specific columns\nfileds = [:salary, :score]\ngroups = DataTreatments.groupby(df, fileds)\n\ndf_norm = DataTreatments.normalize(groups, zscore(); dims=0)\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.zscore","page":"Normalization","title":"DataTreatments.zscore","text":"zscore(; [method::Symbol]) -> Function\nzscore(x::NormType; kwargs...)\n\nCreate a z-score normalization function that standardizes data by centering and scaling.\n\nArguments\n\nmethod::Symbol: Method for computing the z-score\n\nMethods\n\nStandard Z-Score (:std, default)\n\nCenters data to mean 0 and scales to standard deviation 1.\n\nz = fracx - musigma\n\nwhere Î¼ is the mean and Ïƒ is the standard deviation.\n\nRobust Z-Score (:robust)\n\nCenters data to median 0 and scales to median absolute deviation 1. More resistant to outliers than standard z-score.\n\nz = fracx - textmedian(x)textMAD(x)\n\nwhere MAD is the median absolute deviation.\n\nHalf-Normal Z-Score (:half)\n\nNormalizes to the standard half-normal distribution using minimum and half-standard deviation.\n\nz = fracx - min(x)sigma_texthalf\n\nwhere Ïƒ_half = Ïƒ / âˆš(1 - 2/Ï€).\n\nExamples\n\nX = rand(100, 50)\n\n# Standard z-score normalization\nX_norm = DataTreatments.normalize(X, zscore())\n# Result: mean â‰ˆ 0, std â‰ˆ 1\n\n# Robust z-score (resistant to outliers)\nX_robust = DataTreatments.normalize(X, zscore(method=:robust))\n# Result: median â‰ˆ 0, MAD â‰ˆ 1\n\n# Half-normal z-score\nX_half = DataTreatments.normalize(X, zscore(method=:half))\n# Result: minimum â‰ˆ 0, scaled by half-standard deviation\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.sigmoid","page":"Normalization","title":"DataTreatments.sigmoid","text":"sigmoid() -> Function\nsigmoid(x::NormType)\n\nCreate a sigmoid normalization function that maps data to the interval (0, 1).\n\nThe sigmoid (or logistic) function provides a smooth, S-shaped transformation that  maps the entire real line to the bounded interval (0, 1), with the steepest slope  at the mean of the data.\n\nFormula\n\nsigma(x) = frac11 + e^-fracx - musigma\n\nwhere:\n\nÎ¼ (mu) is the mean of the input data\nÏƒ (sigma) is the standard deviation of the input data\nThe output is bounded: 0 < Ïƒ(x) < 1\n\nExample\n\nX = rand(100, 50)\n\n# Sigmoid normalization\nX_sigmoid = DataTreatments.normalize(X, sigmoid())\n# Result: all values in (0, 1), mean(X) â†’ 0.5\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.pnorm","page":"Normalization","title":"DataTreatments.pnorm","text":"pnorm(; [p::Real]) -> Function\npnorm(x::NormType; kwargs...)\n\nCreate a normalization function that scales data by the p-norm.\n\nThe p-norm normalization divides each element by the p-norm of the entire dataset, ensuring that the normalized data has unit p-norm. This is particularly useful for standardizing data magnitudes across different scales.\n\nArguments\n\np::Real: The norm order (default: 2)\np = 1: Manhattan norm (sum of absolute values)\np = 2(default): Euclidean norm (default, root sum of squares)\np = Inf: Infinity norm (maximum absolute value)\np > 0: General p-norm\n\nFormula\n\nGeneral p-norm (p â‰¥ 1):\n\nx_p = left(sum_i=1^n x_i^pright)^1p\n\nExamples\n\nX = rand(100, 50)\n\n# L2 norm (Euclidean, default)\nX_norm = DataTreatments.normalize(X, pnorm())\n# Result: â€–Xâ€–â‚‚ = 1\n\n# L1 norm (Manhattan)\nX_L1 = DataTreatments.normalize(X, pnorm(p=1))\n# Result: sum(abs, X) = 1\n\n# Lâˆž norm (Maximum)\nX_Linf = DataTreatments.normalize(X, pnorm(p=Inf))\n# Result: maximum(abs, X) = 1\n\n# Custom p-norm\nX_L4 = DataTreatments.normalize(X, pnorm(p=4))\n# Result: (sum(X.^4))^(1/4) = 1\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.scale","page":"Normalization","title":"DataTreatments.scale","text":"scale(; [factor::Symbol]) -> Function\nscale(x::NormType; kwargs...)\n\nCreate a normalization function that scales data by a specified scale factor.\n\nScale normalization divides data by a characteristic scale measure, standardizing the spread or magnitude without necessarily centering the data. This is useful when you want to normalize variability but preserve the mean or baseline.\n\nArguments\n\nfactor::Symbol: Scale factor to use\n\nScale Factor Options\n\nStandard Deviation (:std, default)\n\nScale data to have standard deviation of 1.\n\nx_textscaled = fracxsigma\n\nwhere Ïƒ is the standard deviation.\n\nMedian Absolute Deviation (:mad)\n\nScale data to have median absolute deviation of 1.\n\nx_textscaled = fracxtextMAD(x)\n\nwhere MAD = median(|x - median(x)|).\n\nFirst Element (:first)\n\nScale data by the value of the first element.\n\nx_textscaled = fracxx_1\n\nInterquartile Range (:iqr)\n\nScale data to have interquartile range of 1.\n\nx_textscaled = fracxtextIQR(x)\n\nwhere IQR = Qâ‚ƒ - Qâ‚ (75th percentile - 25th percentile).\n\nExamples\n\nX = rand(100, 50)\n\n# Standard deviation scaling (default)\nX_scaled = DataTreatments.normalize(X, scale())\n# Result: std(X_scaled) â‰ˆ 1, mean unchanged\n\nX_outliers = [1, 2, 3, 4, 100]  # Has outlier\n\n# Robust scaling with MAD\nX_mad = DataTreatments.normalize(X_outliers, scale(factor=:mad))\n# More robust than std scaling\n\n# IQR scaling\nX_iqr = DataTreatments.normalize(X, scale(factor=:iqr))\n# Result: IQR(X_iqr) â‰ˆ 1\n\nprices = [100.0, 105.0, 98.0, 110.0]\n\n# Baseline normalization (first element)\nprices_norm = DataTreatments.normalize(prices, scale(factor=:first))\n# Result: [1.0, 1.05, 0.98, 1.10] - percentage of initial price\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.minmax","page":"Normalization","title":"DataTreatments.minmax","text":"minmax(; [lower::Real, upper::Real]) -> Function\nminmax(x::NormType; kwargs...)\n\nCreate a min-max normalization function that rescales data to a specified range.\n\nArguments\n\nlower::Real: Lower bound of the output range (default: 0.0)\nupper::Real: Upper bound of the output range (default: 1.0)\n\nFormula\n\nx_textscaled = textlower + fracx - x_minx_max - x_min cdot (textupper - textlower)\n\nThis maps the original range [xmin, xmax] to [lower, upper] via affine transformation.\n\nExamples\n\nX = rand(100, 50)\n\n# Standard min-max scaling to [0, 1]\nX_norm = DataTreatments.normalize(X, DataTreatments.minmax())\n# Result: min â‰ˆ 0, max â‰ˆ 1\n\n# Custom range scaling to [-1, 1]\nX_scaled = DataTreatments.normalize(X, DataTreatments.minmax(lower=-1.0, upper=1.0))\n# Result: min â‰ˆ -1, max â‰ˆ 1\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.center","page":"Normalization","title":"DataTreatments.center","text":"center(; [method::Symbol]) -> Function\ncenter(x::NormType; kwargs...)\n\nCreate a centering normalization function that shifts data to have zero central tendency.\n\nCentering (also known as mean/median centering or demeaning) translates data by  subtracting a measure of central tendency, shifting the distribution without changing  its spread or shape. This is useful for removing baseline offsets and focusing on  relative deviations.\n\nArguments\n\nmethod::Symbol: Centering method (default: :mean)\n:mean(default): Center around arithmetic mean (subtracts mean)\n:median: Center around median (subtracts median, more robust to outliers)\n\nFormula\n\nMean Centering (:mean, default)\n\nx_textcentered = x - barx\n\nMedian Centering (:median)\n\nx_textcentered = x - textmedian(x)\n\nExamples\n\nX = rand(100, 50)\n\n# Mean centering (default)\nX_centered = DataTreatments.normalize(X, center())\n# Result: mean(X_centered) â‰ˆ 0, std unchanged\n\nX_outliers = [1, 2, 3, 4, 100]  # Has outlier\n\n# Median centering\nX_med = DataTreatments.normalize(X_outliers, center(method=:median))\n# Result: median(X_med) = 0, outlier less influential\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.unitpower","page":"Normalization","title":"DataTreatments.unitpower","text":"unitpower() -> Function\nunitpower(x::NormType)\n\nCreate a normalization function that scales data to have unit root mean square (RMS) power.\n\nUnit power normalization divides each element by the root mean square (RMS) of the  entire dataset, ensuring that the normalized data has RMS = 1. This is commonly used  in signal processing to normalize signal power.\n\nFormula\n\nx_textnormalized = fracxtextRMS(x)\n\nwhere the Root Mean Square (RMS) is:\n\ntextRMS(x) = sqrtfrac1nsum_i=1^n x_i^2 = sqrttextmean(x^2)\n\nExample\n\nX = rand(100, 50)\n\n# Unit power normalization\nX_norm = DataTreatments.normalize(X, unitpower())\n# Result: RMS(X_norm) = 1\n\n\n\n\n\n","category":"function"},{"location":"normalization/#DataTreatments.outliersuppress","page":"Normalization","title":"DataTreatments.outliersuppress","text":"outliersuppress(; [thr::Real]) -> Function\noutliersuppress(x::NormType; kwargs...)\n\nCreate a normalization function that suppresses outliers by capping values beyond a threshold.\n\nOutlier suppression identifies values that deviate more than a specified number of  standard deviations from the mean and replaces them with the threshold boundary value. This technique reduces the influence of extreme values while preserving the sign and  general structure of the data.\n\nArguments\n\nthr::Real=5.0: Threshold in standard deviations (default: 5.0)\n\nThreshold choice\n\nLower thresholds more aggressively modify data\n\nUse thr=0.3 for typical outlier removal (3-sigma rule)\nUse thr=0.5 (default) for conservative outlier handling\n\nFormula\n\nx_textsuppressed = begincases\nmu + textthr cdot sigma  textif  x  mu + textthr cdot sigma \nmu - textthr cdot sigma  textif  x  mu - textthr cdot sigma \nx  textotherwise\nendcases\n\nwhere:\n\nÎ¼ is the mean of the data\nÏƒ is the standard deviation\nValues within [Î¼ - thrÂ·Ïƒ, Î¼ + thrÂ·Ïƒ] remain unchanged\n\nExamples\n\nX = [1, 2, 3, 4, 5, 100]  # 100 is an outlier\n\n# Default threshold (0.5 standard deviations)\nX_suppressed = DataTreatments.normalize(X, outliersuppress())\n# Result: [1, 2, 3, 4, 5, ~mean+5*std] - outlier capped\n\n# More aggressive suppression (3 std)\nX_aggressive = DataTreatments.normalize(X, outliersuppress(thr=0.3))\n# Caps values beyond mean Â± 3*std (more values affected)\n\n\n\n\n\n","category":"function"},{"location":"treatment/#datatreatment","page":"DataTreatment","title":"DataTreatment","text":"","category":"section"},{"location":"treatment/#What-is-a-data-treatment?","page":"DataTreatment","title":"What is a data treatment?","text":"DataTreatments.jl provides tools for manipulating and analyzing multidimensional datasets, meaning datasets whose elements are not single numbers but signals (e.g., audio or time-series sensor inputs) or higherâ€‘dimensional structures (e.g., images and beyond).","category":"section"},{"location":"treatment/#Why-is-data-treatment-necessary?","page":"DataTreatment","title":"Why is data treatment necessary?","text":"Multidimensional data such as audio or images often have very large sizes due to high resolution. In many cases, we cannot perform analysis or machine learning directly on such large data. Therefore, dataâ€‘compression algorithms are useful to reduce dimensionality while minimizing information loss.\n\nOne of the most widely used approaches is windowing.","category":"section"},{"location":"treatment/#Two-common-scenarios","page":"DataTreatment","title":"Two common scenarios","text":"reducesize   The output dataset keeps the same overall structure as the input, but with smaller elements.\nExample (reducesize):\nusing DataTreatments\n\nX = [rand(4) for _ in 1:3, _ in 1:2]\n\nvnames = [:ch1, :ch2];\nwin = splitwindow(nwindows=2);\nfeatures = (mean, maximum);\n\ndt_rs = DataTreatment(X, :reducesize; vnames, win, features);\n\nget_dataset(dt_rs)\naggregate   The dataset is resized and also transformed into a tabular dataset, where windows become consecutive columns in the output.\nExample (reducesize):\nusing DataTreatments\n\nX = [rand(4) for _ in 1:2, _ in 1:2]\n\nvnames = [:ch1, :ch2];\nwin = splitwindow(nwindows=2);\nfeatures = (mean,);\n\ndt_ag = DataTreatment(X, :aggregate; vnames, win, features);\n\nget_dataset(dt_ag)\nget_featureid(dt_ag)","category":"section"},{"location":"treatment/#Note","page":"DataTreatment","title":"Note","text":"Windowing is especially useful for normalizing datasets whose elements have different sizes (common with audio files). By using a fixed number of windows, object sizes are normalized and subsequent analysis becomes more reliable.","category":"section"},{"location":"grouping/#grouping","page":"Grouping","title":"Grouping","text":"","category":"section"},{"location":"grouping/#Why-group-features?","page":"Grouping","title":"Why group features?","text":"Often you work with datasets whose columns have different units of measurement. A simple example is a medical dataset where we have audio files together with recordings of electromagnetic pulses.\n\nSeveral machine learning algorithms explicitly require the input data to be normalized (typically in a 0â€“1 range). We could normalize column by column, but this risks flattening the dataset with a consequent loss of information.\n\nThat is why the possibility to group columns according to a certain logic is useful.\n\nThe DataTreatments package provides this functionality not only for multidimensional datasets, but also for tabular datasets.","category":"section"},{"location":"grouping/#DataTreatments.groupby-Tuple{DataTreatment, Vararg{Symbol}}","page":"Grouping","title":"DataTreatments.groupby","text":"groupby(data::DataTreatment, fields...)\n\nGroup rows in a DataTreatment dataset by one or more feature attributes.\n\nPurpose\n\nThe groupby function enables hierarchical grouping of DataTreatment columns based on  feature properties stored in the FeatureId structure. This is essential for operations  that require consistent computation across groups rather than column-by-column, preventing  data inconsistencies and unwanted flattening.\n\nFeatureId Structure\n\nEach feature in the dataset carries metadata through its FeatureId:\n\nvname: The name/identifier of the feature\nnwin: The window number associated with multidimensional levels\nfeat: The feature used to reduce or aggregate multidimensional elements to  manageable computational size\n\nUse Cases\n\nNormalization Across Groups\n\nInstead of normalizing column-by-column (which can flatten or damage the dataset),  you can compute normalization coefficients spanning an entire group, ensuring  consistent scaling across related features.\n\nExample: Normalize all channels of a multi-channel sensor reading together,  rather than independently per channel.\n\nMulti-Level Grouping\n\nGroup hierarchically by multiple attributes (e.g., first by feature name, then by  window, then by reduction method) for complex data analysis pipelines.\n\nReturns\n\nTuple of:\n\ngroups: Vector of index groups mapping to original dataset positions\nfeat_groups: Corresponding FeatureId groups for each group of indices\n\n\n\n\n\n","category":"method"},{"location":"grouping/#DataTreatments.groupby-Tuple{DataFrames.DataFrame, Vector{Vector{Symbol}}}","page":"Grouping","title":"DataTreatments.groupby","text":"groupby(df::DataFrame, fields::Vector{Vector{Symbol}})\n\nGroup DataFrame columns into multiple sub-DataFrames based on pre-defined column groups.\n\nArguments\n\ndf: Source DataFrame.\nfields: Vector of column-name vectors (each inner vector is one group).\n\nBehavior\n\nComputes any leftover columns not listed in fields and appends them as a final group.\nReturns a Vector{DataFrame} where each element is df[!, fields[i]].\n\nExample\n\nusing DataFrames\n\ndf = DataFrame(\n    sepal_length = rand(3),\n    sepal_width  = rand(3),\n    petal_length = rand(3),\n    petal_width  = rand(3),\n)\n\nfields = [[:sepal_length, :petal_length], [:sepal_width]]\ngroups = groupby(df, fields)\n\n\n\n\n\n","category":"method"},{"location":"#DataTreatments.jl","page":"Home","title":"DataTreatments.jl","text":"A Julia package for processing datasets containing multidimensional elements through windowing and dimensionality reduction techniques.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"DataTreatments.jl provides tools for working with matrices, or DataFrames where each element is itself a multidimensional object (vectors, matrices, or higher-dimensional arrays). It offers:\n\nWindowing functions for partitioning multidimensional data\nDimensionality reduction using feature extraction together with windowing\nTabular transformation through feature extraction to convert complex multidimensional datasets into flat feature matrices suitable for standard machine learning models\nData normalization with multiple methods (z-score, min-max, sigmoid, etc.) for preprocessing\nGroup-wise operations via groupby for consistent processing across related features\nComplete reproducibility by storing all processing parameters and feature metadata\n\nThis package is particularly useful when you need to apply traditional ML algorithms that require tabular input to datasets containing structured multidimensional elements like images, spectrograms, or time series segments.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"DataTreatments\")","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#Basic-Usage-with-Matrix","page":"Home","title":"Basic Usage with Matrix","text":"using DataTreatments\n\n# Create a dataset with multidimensional elements\nXmatrix = [rand(1:100, 4, 2) for _ in 1:10, _ in 1:5]  # 10Ã—5 dataset where each element is a 4Ã—2 matrix\nvnames = Symbol.(\"auto\", 1:5)\n\n# Define processing parameters\nwin = splitwindow(nwindows=2)\nfeatures = (mean, std, maximum, minimum)\nnorm = zscore()\nreducefunc = median\n\n# Process for propositional analysis\nresult = DataTreatment(Xmatrix, :aggregate; vnames, win, features, norm)\n\n# Process for modal analysis\nresult = DataTreatment(Xmatrix, :reducesize; vnames, win, features, reducefunc, norm)","category":"section"},{"location":"#Basic-Usage-with-DataFrame","page":"Home","title":"Basic Usage with DataFrame","text":"using DataTreatments\nusing DataFrames\n\n# Create dataset with multidimensional elements\ndf = DataFrame(\n    channel1 = [rand(200, 120) for _ in 1:1000],\n    channel2 = [rand(200, 120) for _ in 1:1000],\n    channel3 = [rand(200, 120) for _ in 1:1000]\n)\n\n# Define processing parameters\nwin = adaptivewindow(nwindows=6, overlap=0.15)\nfeatures = (mean, std, maximum, minimum, median)\nnorm = pnorm(p=1)\nreducefunc = median\n\n# Process for propositional analysis\nresult = DataTreatment(df, :aggregate; win, features, norm)\n\n# Process for modal analysis\nresult = DataTreatment(df, :reducesize; win, features, reducefunc, norm)\n\n# Access processed data\nX_flat = get_dataset(result)        # Flat feature matrix\nfeature_ids = get_featureid(result) # Feature metadata","category":"section"},{"location":"#Core-Concepts","page":"Home","title":"Core Concepts","text":"","category":"section"},{"location":"#Windowing-Functions","page":"Home","title":"Windowing Functions","text":"DataTreatments provides several windowing strategies:","category":"section"},{"location":"#splitwindow-Equal-Non-Overlapping-Windows","page":"Home","title":"splitwindow - Equal Non-Overlapping Windows","text":"win = splitwindow(nwindows=3)\n# Divides data into 3 equal, non-overlapping segments","category":"section"},{"location":"#movingwindow-Fixed-Size-Sliding-Windows","page":"Home","title":"movingwindow - Fixed-Size Sliding Windows","text":"win = movingwindow(winsize=50, winstep=25)\n# Creates overlapping windows of size 50, advancing by 25 points","category":"section"},{"location":"#adaptivewindow-Windows-with-Controlled-Overlap","page":"Home","title":"adaptivewindow - Windows with Controlled Overlap","text":"win = adaptivewindow(nwindows=5, overlap=0.2)\n# Creates 5 windows with 20% overlap between consecutive windows","category":"section"},{"location":"#wholewindow-Single-Window-(Entire-Dimension)","page":"Home","title":"wholewindow - Single Window (Entire Dimension)","text":"win = wholewindow()\n# Creates a single window covering the entire dimension","category":"section"},{"location":"#Multi-Dimensional-Windowing","page":"Home","title":"Multi-Dimensional Windowing","text":"Use the @evalwindow macro to apply window functions to each dimension:\n\nX = rand(200, 120)\n\n# Apply same windowing to all dimensions\nintervals = @evalwindow X splitwindow(nwindows=4)\n\n# Apply different windowing per dimension\nintervals = @evalwindow X splitwindow(nwindows=4) movingwindow(winsize=40, winstep=20)","category":"section"},{"location":"#Normalization-Functions","page":"Home","title":"Normalization Functions","text":"DataTreatments provides flexible normalization strategies for different data structures:","category":"section"},{"location":"#Normalization-Methods","page":"Home","title":"Normalization Methods","text":"const DT = DataTreatments\n\n# Z-score normalization (mean=0, std=1)\nDT.zscore()                    # Standard z-score\nDT.zscore(method=:robust)      # Robust z-score (median, MAD)\nDT.zscore(method=:half)        # Half-normal z-score\n\n# Min-max scaling\nDT.minmax()                    # Scale to [0, 1]\nDT.minmax(lower=-1, upper=1)   # Scale to custom range\n\n# Sigmoid transformation\nDT.sigmoid()                   # Logistic sigmoid to (0, 1)\n\n# Centering\nDT.center()                    # Center to mean=0\nDT.center(method=:median)      # Center to median=0\n\n# Scaling\nDT.scale()                     # Scale to std=1\nDT.scale(factor=:mad)          # Scale by MAD\n\n# Power normalization\nDT.unitpower()                 # Normalize to unit RMS power\n\n# P-norm normalization\nDT.pnorm(p=2)                  # L2 normalization (unit length)\n\n# Outlier suppression\nDT.outliersuppress(thr=3.0)    # Cap outliers beyond threshold","category":"section"},{"location":"#Element-wise-Normalization","page":"Home","title":"Element-wise Normalization","text":"Normalize using global statistics across all elements:\n\nX = rand(100, 50)  # 100 samples, 50 features\n\n# Z-score: mean â‰ˆ 0, std â‰ˆ 1 globally\nX_norm = DT.normalize(X, DT.zscore())\n\n# Time series dataset: each element is a time series\nX = [rand(100) for _ in 1:50, _ in 1:3]  # 50 samples Ã— 3 channels\n\n# Each channel normalized across all 50 time series\nX_norm = DT.normalize(X, DT.zscore())","category":"section"},{"location":"#Tabular-Normalization","page":"Home","title":"Tabular Normalization","text":"Normalize each column or row independently:\n\nX = rand(100, 50)  # 100 samples, 50 features\n\n# Column-wise (each feature normalized independently)\nX_norm = DT.normalize(X, zscore(); dims=2)\n\n# Row-wise (each sample normalized independently)\nX_norm = DT.normalize(X, zscore(); dims=1)","category":"section"},{"location":"#Grouping-Functions","page":"Home","title":"Grouping Functions","text":"Grouping lets you partition related feature columns (e.g., by variable name, window, or feature) so that operations like normalization are applied with shared coefficients across each group instead of per-column. This preserves consistent scaling for semantically related parts of the dataset.\n\ndt = DataFrame([rand(1:100, 4, 2) for _ in 1:10, _ in 1:5], :auto)\nwin = splitwindow(nwindows=2)\n\ngrp1 = [:x1, :x2]\n\ngroups = DT.groupby(dt, grp1)\n\ndt_norm = DataTreatment(dt, :aggregate; win, features, groups=(:vname,), norm=DT.zscore())","category":"section"},{"location":"#Data-Structures","page":"Home","title":"Data Structures","text":"","category":"section"},{"location":"#FeatureId-Feature-Metadata","page":"Home","title":"FeatureId - Feature Metadata","text":"A metadata container that stores information about each feature column for reproducibility and feature selection:\n\n# Created automatically by DataTreatment\ndt = DataTreatment(df, :reducesize; win=(win,), features=(mean, std, maximum))\n\n# Access feature metadata\nfeature_ids = get_featureid(dt)\n\n# Each FeatureId contains:\n# - vname: Source variable name\n# - feat: Feature function applied\n# - nwin: Window number","category":"section"},{"location":"#DataTreatment-Complete-Processing-Container","page":"Home","title":"DataTreatment - Complete Processing Container","text":"A comprehensive container that stores processed data along with all parameters for full reproducibility:\n\n# Create dataset\ndf = DataFrame(\n    channel1 = [rand(200, 120) for _ in 1:1000],\n    channel2 = [rand(200, 120) for _ in 1:1000],\n    channel3 = [rand(200, 120) for _ in 1:1000]\n)\n\n# Process with full parameter storage\nwin = adaptivewindow(nwindows=6, overlap=0.15)\nfeatures = (mean, std, maximum, minimum, median)\n\ndt = DataTreatment(df, :reducesize; win, features, norm=DT.minmax())\n\n# Access processed data\nX_flat = get_dataset(dt)        # Flat feature matrix\nfeature_ids = get_featureid(dt) # Feature metadata\n\n# All parameters are stored for reproducibility\naggrtype = get_aggrtype(dt)     # :reducesize\nreduction = get_reducefunc(dt)   # mean (default)\nvar_names = get_vnames(dt)       # [:channel1, :channel2, :channel3]\nfeat_funcs = get_features(dt)    # (mean, std, maximum, minimum, median)\nn_windows = get_nwindows(dt)     # 6","category":"section"},{"location":"#Use-Cases","page":"Home","title":"Use Cases","text":"Audio Processing: Extract features from spectrograms for audio classification\nImage Analysis: Process image patches for computer vision tasks\nTime Series: Analyze segmented multivariate time series\nSignal Processing: Extract statistical features from signal windows\nMedical Data: Process multi-channel physiological signals\nExperiment Reproducibility: All parameters stored for exact replication","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"},{"location":"#About","page":"Home","title":"About","text":"Developed by the ACLAI Lab @ University of Ferrara.","category":"section"}]
}
